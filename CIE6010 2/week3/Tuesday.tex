
\chapter{Week3}

\section{Wednesday}\index{week3_Tuesday_lecture}
Assignment 2 posted.

CIE6010: Exercise 1.2.9 and 1.3.9; together with MATLAB project.

\subsection{Convex Analysis}
Last time we have shown that for a unconstrained problem, $\nabla f(\bm x)=0$ is the necessary and sufficient condition for global minimum ensurance. However, the case for constrained problem will be different.
\begin{proposition}
For the \emph{constrained} problem
\[
\begin{array}{ll}
\min&\quad f(x)\\
&\bm x\in X\subseteq\mathbb{R}\\
&f\mbox{ is convex in }\mathcal{C}^1
\end{array}
\]
$\bm x$ is a global minimum iff \[
\inp{\nabla f(\bm x)}{\bm y - \bm x}\ge0
\]
for $\forall\bm y\in X$.
\end{proposition}
\begin{proof}
Since $f$ is convex, the inequality below holds:
\[
f(\bm y)\ge f(\bm x)+\inp{\nabla f(\bm x)}{\bm y - \bm x},\forall \bm x,\bm y\in X
\]
Note that $\bm x$ is a global minimum iff $f(\bm y)\ge f(\bm x)$, $\forall \bm y\in X$. Combining the inequality above, the proof is complete.
\end{proof}
\begin{remark}
\begin{itemize}
\item
Such a condition is not so useful unless $\bm y$ lies in the whole space, at that time we have no choice but $\nabla f(\bm x)=\bm0$. (otherwise we can construct a $\bm y$ to let the inner product negative.)
\item
An equivalent version of the condition is that every \emph{feasible} direction is \emph{ascending}.
\end{itemize}
\end{remark}

\begin{definition}[Descending Direction]
The vector $\bm d\in\mathbb{R}^n$ is said to be a \emph{descending direction} of $f$ at $\bm x$ if
\[
\inp{\nabla f(\bm x)}{\bm d}<0.
\]
\end{definition}
This definition is the motivation of descent method.

\subsection{Iterative Method}
\begin{definition}[Descent Method]
At any non-stationary $\bm x$, i.e., $\nabla f(x)\ne\bm0$, we find the descenting direction $d$, i.e., $\inp{\nabla f(\bm x)}{\bm d}<0$. We update our old $\bm x$ as:
\[
\bm x^{r+1}\leftarrow \bm x^r+\alpha^r\bm d^r,\quad \alpha>0.
\]
\end{definition}
The key is how to choose $\bm d$ and $\alpha$. We have a general formula for $\bm d$:
\[
\bm d = -\bm D\cdot\nabla f(\bm x),
\]
where $\bm D\in\mathbb{S}^n$ and $\bm D\succ0$. (Verify by yourself that $\bm d$ satisfies the descending direction definition)
\begin{enumerate}
\item
$\bm D=\bm I$ implies gradient method (Steepest Descent).
\item
$\bm D = \left(\nabla^2 f(\bm x)\right)^{-1}$ implies the Newton's method.
\end{enumerate}

\paragraph{Nonlinear LS} The optimization problem is
\[
\begin{array}{ll}
\min&\quad f(\bm x) = \frac{1}{2}\sum_{i=1}^mg_i^2(\bm x):=\frac{1}{2}\|g(\bm x)\|_2^2\\
&g(\bm x)=\begin{pmatrix}
g_1(\bm x)&g_2(\bm x)&\cdots&g_m(\bm x)
\end{pmatrix}\trans
\end{array}
\]

The gredient function is
\begin{align*}
\nabla f(\bm x)&=\sum_{i=1}^mg_i(\bm x)\nabla g_i(\bm x)\\
&=
\underbrace{\begin{bmatrix}
\nabla g_1(\bm x)&\cdots&\nabla g_m(\bm x)
\end{bmatrix}}_{\nabla g(\bm x)\in\mathbb{R}^{n\times m}}\begin{bmatrix}
g_1(\bm x)\\\vdots\\ g_m(\bm x)
\end{bmatrix}\\&=
\nabla g(\bm x)\cdot g(\bm x)\\
&=\inp{\bm J(\bm x)}{g(\bm x)},
\end{align*}
where $J(\bm x)\in\mathbb{R}^{m\times n}$ is said to be the Jacobian matrix of $g(\bm x)$.

The second order derivative function is given as: (complete the calculation process by yourself)
\[
\nabla^2 f(\bm x) = \bm J\trans(\bm x)\bm J(\bm x)+\sum_{i=1}^mg_i(\bm x)\nabla^2 g_i(\bm x),
\]
the second term in RHS is complicated and hard to compute. To solve this LS problem, the Gauss-Newton method directly ignore it, which leads to the descent direction
\[
\bm d = -(\bm J\trans\bm J)^{-1}\bm J\trans g(\bm x)
\]
\paragraph{Choice of Step Length $\alpha$}
We apply the Limited Minimization Rule to find $\alpha$, i.e., for fixed $s>0$, choose $\alpha^r$ such that
\[
\min_{\alpha^r\in(0,s]}f(\bm x^r+\alpha^r\bm d^r).
\]
Usually this rule is too computationally expensive. The alternative ways are:
\begin{itemize}
\item
Choose $\alpha$ just as a constant
\item
Choose $\alpha^r\to0$ as $r\to\infty$ but also satisfies the infinite travel condition
\[
\sum_{r=0}^\infty\alpha^r=\infty
\]
\end{itemize}
Adding Lipschitz condition will make the choice of step-length easier:
\begin{definition}[Lipschitz Continuous]
$\nabla f$ is \emph{Lipschitz continuous} with Lipschitz constant $L$ if
\[
\|\nabla f(\bm x) - \nabla f(\bm y)\|\le L\|\bm x-\bm y\|
\]
for all $\bm x,\bm y$.
\end{definition}
\begin{remark}
\begin{itemize}
\item
It is useful to note that convexity places a lower bound on the growth of the function at every point; whereas Lipschitzness places a upper bound on the growth of the function that is linear in the perturbation i.e., $\|\bm x-\bm y\|_2$. Also note that Lipschitz functions need not be differentiable. However, differentiable functions with bounded gradients are always Lipschitz.
\item
The Lipshitz conditon induces that for iterative method we have
\[
f(\bm x^r) - f(\bm x^{r+1})\ge\frac{L}{2}\|\nabla f(\bm x^r)\|^2.
\]
From this inequality, we imply that the result of itervative convergence is $\nabla f(\bm x^r)\to0$, but the minimum point is still un-guaranteed. In Deep Learing people often train the data using this way, which is not so rigorous.
\end{itemize}

\end{remark}

\paragraph{Convergence Rate Analysis}
We apply the Lipschitzness to analysis the rate of convergence first. Setting $h(t) = f(\bm x+t\alpha\bm d)$, we find that
\begin{align*}
f(\bm x+\alpha\bm d) - f(\bm x)&=h(1) - h(0)=
\int_0^1
h'(t)\diff t\\
&=
\int_0^1
\inp{\nabla f(\bm x+t\cdot\alpha\bm d)}{\alpha \bm d}\diff t\\
&=
\int_0^1\left[
\inp{\nabla f(\bm x+t\cdot\alpha\bm d)}{\alpha \bm d}
-
\inp{\nabla f(\bm x)}{\alpha\bm d}
+
\inp{\nabla f(\bm x)}{\alpha\bm d}
\right]\diff t\\
&=\inp{\nabla f(\bm x)}{\alpha\bm d}+\int_0^1
\inp{\nabla f(\bm x+t\cdot\alpha\bm d) - \nabla f(\bm x)}{\alpha \bm d}
\diff t\\
&\le
\inp{\nabla f(\bm x)}{\alpha\bm d}
+
\int_0^1\|\nabla f(\bm x+t\cdot \alpha\bm d) - \nabla f(\bm x)\|\cdot\|\alpha\bm d\|\diff t\\
&\le \inp{\nabla f(\bm x)}{\alpha\bm d}
+L\int_0^1t\alpha^2\|\bm d\|^2\diff t\\
&=\underbrace{\inp{\nabla f(\bm x)}{\alpha\bm d}}_{\text{negative}}+\frac{L\alpha^2\| d\|^2}{2}
\end{align*}

\paragraph{Choice of Step Length}
To get the optimal step length $\alpha$, differentiating the RHS w.r.t. $\alpha$ leads to
\[
\inp{\nabla f(\bm x)}{\bm d}+L\alpha\|\bm d\|^2=0\implies
\alpha=-\frac{\inp{\nabla f(\bm x)}{\bm d}}{L\|\bm d\|^2}>0,
\]
which seems a reasonable choice. If $\bm d$ is the steepest descent direction, the step-length becomes a constant:
\[
\alpha=\frac{1}{L}.
\]












