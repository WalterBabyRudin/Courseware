
\chapter{Week9}
\section{Monday}\index{week2_Tuesday_lecture}
\paragraph{Annonuncement}
The exam is on Wednesday, from 10:30 to 12:00 in F302, Shaw College. The exam will be quite different frome previous years, since some of you search all the tests the lecturer given online. KKT are the most important one in the test.
\subsection{Reviewing for KKT}
\begin{equation}\label{Eq:9:1}
\begin{array}{ll}
\min&f(x)\\
&h_i(x)=0,\quad i=1,2,\dots,m<n\\
&g_i(x)\le0,\quad i=1,2,\dots,r
\end{array}
\end{equation}
The Lagrange function is given by:
\begin{align*}
L(x,\lambda,\mu)&=f(x)+\lambda\trans h(x)+\mu\trans g(x)
\end{align*}
The active set is the set of indices such that the inequalities are satisfied:
\[
A(x)=\{i\mid g_i(x)=0, 1\le i\le r\}
\]
The pre-assumption is the constraint qualification (QC), or called \emph{regularity}. At the local minimum point $x^*$, we have
\[
\{\nabla h_i(x^*),\forall i, \nabla g_i(x^*),\forall i\in A(x^*)\}\mbox{ are linearly independent}
\]
Under differentiability and a QC, we have KKT (necessary) conditions:
\begin{proposition}[First Order KKT Conditions]
For local minimum $x^*$ of (\ref{Eq:9:1}) that is differentiable, we have
\begin{enumerate}
\item
$h(x)=0,g(x)\le0$
\item
$\nabla_xL(x,\lambda,\mu)=0,\mu\ge0$
\item
$\mu\circ g(x)=0$
\end{enumerate}
\end{proposition}
\begin{proposition}[Second Order KKT Conditions]
For local minimum $x^*$ of (\ref{Eq:9:1}) that is twice differentiable, we have
\begin{enumerate}
\item
$\bm y\trans\nabla^2_{xx}L(x,\lambda,\mu)\bm y\ge0$, for $\forall \bm y\in V(x^*)$ with
\[
V(x^*)=\{\bm y\mid \nabla\trans h_i(x)\bm y=0,\forall i; \nabla\trans g_i(x)\bm y=0, \forall i\in A(x^*)\}
\]

\end{enumerate}
\end{proposition}
During the exam, the counter-example for KKT conditon is given (the pre-assumption of QC is not met).
\begin{example}
\[
\begin{array}{ll}
\min&x_1\\
&x_2-x_1^2\le0\\
&-x_1\le0
\end{array}
\]
The Lagrange funcion is
\[
L(x,\mu)=x_1+\mu_1(x_2-x_1^2)+\mu_2(-x_1)
\]
\[
\nabla_xL(x,\mu)=\begin{pmatrix}
1\\0
\end{pmatrix}+\mu_2\begin{pmatrix}
-2x_1\\1
\end{pmatrix}+\mu_2\begin{pmatrix}
-1\\0
\end{pmatrix}=\begin{pmatrix}
0\\0
\end{pmatrix},\qquad \mu_1,\mu_2\ge0
\]
which follows that
\[
\mu_1=0,\mu_2=1,\mu_2(-x_1)=0\implies x_1=0.
\]
\end{example}
\begin{example}
\[
\begin{array}{ll}
\min&x_2\\
&x_2-x_1^2\le0\\
&-x_2\le0
\end{array}
\]
The Lagrange function is 
\[
L(x,\mu)=x_2+\mu_1(x_2-x_1^2)+\mu_2(-x_2)
\]
\[
\nabla_xL=\begin{pmatrix}
0\\1
\end{pmatrix}+\mu_1\begin{pmatrix}
-2x_1\\1
\end{pmatrix}+\mu_2\begin{pmatrix}
0\\-1
\end{pmatrix}=\begin{pmatrix}
0\\0
\end{pmatrix}
\]
In this case, the regularity condition is not satisfied. Let's try to solve for this question first:
\[
1+\mu_1-\mu_2=0\implies
\mu_2\ge1,x_2=0
\]
\[
\nabla_{xx}^2L=\mu_1\begin{bmatrix}
-2&0\\0&0
\end{bmatrix}\left\{
\begin{aligned}
=0,&\quad x_2<x_1^2\\
\mbox{negative semi-definite},&\quad x_2=x_1^2
\end{aligned}
\right.
\]
However, the condition for NSD case, $y\trans\nabla_{xx}^2L y\ge0$ for $y\in V(x^*)$ must be true.
\end{example}
\paragraph{Constraint Qualifications}
\begin{enumerate}
\item
Regularity (LICQ)
\item
Slater Condition: If equality constraint $h(x)$ is affine, $g_i$'s are convex, there exists $x$ such that $h(x)=0,g(x)<0$.
\item
Linear Constraints: All $h$ and $g$ are linear
\item
MFCQ: Proposition 4.3.8. in the book
\end{enumerate}

\begin{remark}
If $f$ is convex, then all KKT conditions are sufficient.
\end{remark}
\section{Monday Tutorial: Reviewing for Mid-term}
\paragraph{How to answer sufficient or necessary conditions during the exam? }First derive some inituitive and weak conditons; and then strengthen it by some properties.
\paragraph{Something highlight during the exam}How to compute the dual for specific problems? How to derive the KKT condition? How to compute the gradient? How to dervie iterative formula for some specific problems? What's the convergence rate of iterative algorithms based on which pre-assumptions? e.g., Newton's method has quadratic convergence once the lagrange function is Lipschitz continuous and Hessian matrix is non-singular. What's the proof for the convergence rate?
















