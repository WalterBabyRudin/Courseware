
\section{Friday}\index{week3_Friday_lecture}
\subsection{Review}
\begin{proposition}\label{solution_for_undetermined_system}
Undetermined system $\bm{Ax} = \bm b$ with $m<n$, i.e., number of equations < number of unknowns, has \emph{no solution} or \emph{infinitely many solutions}.
\end{proposition}
We want to understand the meaning of rank: number of ''real'' equations. 

Then we introduce definition of \textit{linearly independence} and \textit{linearly dependence.}

The linear dependence has relation with the system:
\begin{proposition}\label{Pro:3:3}
$\bm{Ax} = \bm 0$ has nonzero solutions if and only if the column vectors of $\bm A$ are dep.
\end{proposition}
Combining proposition (\ref{Pro:3:3}) with (\ref{solution_for_undetermined_system}), 
we derive the corollary:
\begin{corollary}
Any $(n+1)$ vectors in $\mathbb{R}^{n}$ are dep.
\end{corollary}
\begin{proposition}
Undetermined system $\bm{Ax} = \bm b$ with $m\ge n$, i.e., number of equations $\ge$ number of unknowns may have \emph{no solution} or \emph{unique solution} or \emph{infinitely many solutions}.
\end{proposition}
From this proposition we derive the corollary immediately:
\begin{corollary}
Any $(n-1)$ vectors in $\mathbb{R}^{n}$ cannot span the whole space.
\end{corollary}
Then we introduce the definition of basis:
\begin{definition}[Basis]
A set of ind. vectors that span this space is called the \emph{basis} of this space.
\end{definition}
Then we introduce a theorem saying that \emph{All basis of a given vector space have the same size.}

Thus we introduce \emph{dimension} to denote the \textit{number of vectors in a basis.}
\subsection{More on basis and dimension}
The basis of a given vector space has to satisfy two conditions:
\[
\underbrace{\mbox{linear independence}}_{\mbox{not too many}} + 
\underbrace{\mbox{span the space}}_{\mbox{not too few}}
\]

The \emph{ind.} constraint let the size of basis not too many. For example, if given $1000$ vectors of $\mathbb{R}^{3}$, they are very likely to be dep.

\emph{Spanning the space} let the size of basis not too few. For example, given only $3$ vectors of $\mathbb{R}^{100}$, they cannot span the whole space obviously.

We claim that: 
\begin{align*}
\text{A basis }&=\text{ maximal ind. set}\\
&=\text{ minimal spanning set}
\end{align*}
\begin{definition}[spanning set]
$v_1,v_2,\dots,v_n$ is said to be the spanning set of $\bm V$ if 
\[
\bm V = \Span\{v_1,v_2,\dots,v_n\}.
\]
\end{definition}
\begin{example}
$v_1 = \begin{pmatrix}
1\\2\\1
\end{pmatrix}$ is not a basis of $\mathbb{R}^{3}$.

We can add $v_2 = \begin{pmatrix}
1\\0\\0
\end{pmatrix}$, which is ind. of $v_1$. But $v_1,v_2$ still don't form a basis.

If we add one more vector $v_3 = \begin{pmatrix}
0\\1\\0
\end{pmatrix}$, then $v_1,v_2,v_3$ form a basis of $\mathbb{R}^{3}$.
\end{example}

\begin{theorem}\label{theorem_3.3}
Let $\bm V$ be a space of dimension $n>0$, then
\begin{enumerate}
\item
Any set of $n$ ind. vectors span $\bm V$.
\item
Any $n$ vectors that span $\bm V$ are ind.
\end{enumerate}
\end{theorem}

Here is the proof outline, but you should complete the proof in detail.
\begin{proof}[proofoutline.]
\begin{enumerate}
\item
Suppose $v_1,v_2,\dots,v_n$ are ind. and $v$ is an arbitarary vector in $\bm V$. Firstly, show that $v_1,v_2,\dots,v_n,v$ is dep., thus derive the equation $c_1v_1+c_2v_2+\dots+c_{n}v_n+c_{n+1}v = \bm 0$. Argue that the scalar $c_{n+1}\ne 0$. Then we can express $v$ in form of $v_1,v_2,\dots,v_n$, i.e., $v_1,v_2,\dots,v_n$ span $\bm V$.
\item
Suppose $v_1,v_2,\dots,v_n$ span $\bm V$. Assume $v_1,v_2,\dots,v_n$ are dep. Then show that $v_n$ could be written as form of other $(n-1)$ vectors, it follows that $v_1,v_2,\dots,v_{n-1}$ still span $\bm V$. If $v_1,v_2,\dots,v_{n-1}$ are also dep, we can continue eliminating one vector. We continue this way until we get an ind. spanning set with $k<n$ elements, which contradicts dim($\bm V$)$=n$. Therefore, $v_1,v_2,\dots,v_n$ must be ind.
\end{enumerate}
\end{proof}
\begin{example}\label{span_dimension_three}
$\begin{pmatrix}
1\\1\\2
\end{pmatrix},\begin{pmatrix}
2\\1\\3
\end{pmatrix},\begin{pmatrix}
1\\3\\2
\end{pmatrix}$ are ind. $\implies$ they span $\mathbb{R}^{3}$.
\end{example}
\subsubsection{Clarification of dimension}
Firstly, we need to understand ``set'':
\begin{enumerate}
\item
$P\triangleq\{\text{All polynomials}\} = \text{span}\{1,x,x^2,\dots\}\implies \text{dim}(P)=\infty$.
\item
$P_3\triangleq\{\text{All polynomials with degree $\le$ 3}\} = \text{span}\{1,x,x^2,x^3\}\implies \text{dim}(P)=4$.
\item
$Q\triangleq\text{span}\{x^2,1+x^3+x^{10},x^{300}\}\implies \text{dim}(Q) = 3.$
\end{enumerate}
\begin{remark}
dim of space $\ne$ dim of the space it lives in.\\
For example, the line in $\mathbb{R}^{100}$ has dim $1$.
\end{remark}
\subsection{What is rank?}
\begin{definition}[Rank]
The rank of matrix $\bm A$ is defined as the \emph{number of nonzero pivots of rref of} $\bm A$.
\end{definition}
\begin{example}
\[
\bm A = \begin{bmatrix}
1&3&3&4\\2&6&9&7\\-1&-3&3&4
\end{bmatrix}
\xLongrightarrow{\text{row transform}}
\bm U = \begin{bmatrix}
1&3&0&-1\\0&0&1&1\\0&0&0&0
\end{bmatrix}
\]
$\bm U$ has two pivots, hence rank($\bm A$) = rank($\bm U$) = 2.
\end{example}
However, the definition for rank is too complicated, can we define rank of $\bm A$ directly?

\emph{Key question: What quantity is not changed under row transformation?}

\textit{Answer: }Dimension of row space.
\begin{definition}[column space]
The \emph{column space} of a matrix is the subspace of $\mathbb{R}^{n}$ spanned by the columns.

In other words, suppose $\bm A = \left[\begin{array}{c|c|c}
a_1&\dots&a_n
\end{array}\right]$, the column space of $\bm A$ is given by
\[
\mathcal{C}(\bm A) =\Span\{a_1,a_2,\dots,a_n\}.
\]
\end{definition}
\begin{definition}[row space]
The \emph{row space} of a matrix is the subspace of $\mathbb{R}^{n}$ spanned by the rows.

Suppose $\bm A = \left[\begin{array}{c}
a_1\\
\hline
\dots\\
\hline
a_n
\end{array}\right]$, the row space of $\bm A$ is given by
\[
\mathcal{R}(\bm A) = \Span\{a_1,a_2,\dots,a_n\}.
\]

The \emph{row space} of $\bm A$ is essentially $\mathcal{R}(\bm A):=\mathcal{C}(\bm A\trans)$, i.e., the column space of $\bm A\trans$.
\end{definition}
\begin{proposition}\label{row_transformation}
Row transforamtion doesn't change the row space
\end{proposition}
\begin{proof}
After row transformation, \emph{new rows are linear combinations of old rows.}

Hence we have $\mathcal{R}(\mbox{new rows}) \subset\mathcal{R}(\mbox{old rows})$.

More specifically, assuming $\bm A\xLongrightarrow{\text{Row Transfom}}\bm B$, then we have $\mathcal{R}(\bm B)\subset\mathcal{R}(\bm A)$.

Since row transformations are invertible, we also have $\bm B\xLongrightarrow{\text{Row Transfom}}\bm A$, thus we have $\mathcal{R}(\bm A)\subset\mathcal{R}(\bm B)$.

In conclusion, we obtain $\mathcal{R}(\bm B)=\mathcal{R}(\bm A)$.
\end{proof}

Hence $\rank(\bm A) = \text{pivots of $\bm U$} = \dim(\row(\bm U)) = \dim(\row(\bm A))$.

Hence we have a much simpler definition for rank:
\begin{definition}[rank]
The \emph{dimension of the row space} is the \emph{rank of a matrix}, i.e.,
\[
\rank(\bm A) = \dim(\mathcal{R}(\bm A)).
\]
\end{definition}

In the example (\ref{span_dimension_three}), we find $\dim(\row(\bm A)) = \dim(\col(\bm A)) = 2$, is this a coincidence? \textit{The fundamental theorem of linear algebra} gives this answer:
\begin{theorem}\label{column_rank_row_rank}
The row space and column space both have the \emph{same} dimension $\bm r$. 

We call $\dim(\mathcal{C}(\bm A))$ as \textit{column rank}; $\dim(\mathcal{R}(\bm A))$ as \textit{row rank}. 

In brevity, \emph{column rank=row rank= rank}, i.e.,
\[
\begin{array}{ll}
\dim(\mathcal{C}(\bm A)) = \dim(\mathcal{R}(\bm A))=\rank(\bm A),
&
\mbox{for matrix $\bm A$}
\end{array}
\]
\end{theorem}
Let's discuss an example to have an idea of proving it.
\begin{example}
\qquad\\
\[
\bm A = \begin{bmatrix}
1&3&3&4\\2&6&9&7\\-1&-3&3&4
\end{bmatrix}
\xLongrightarrow{\text{row transform}}
\bm U = \begin{bmatrix}
1&3&0&-1\\0&0&1&1\\0&0&0&0
\end{bmatrix}
\]
We notice that \emph{column rank of $\bm A =\bm 2$} and \emph{column rank of $\bm U =\bm 2$}.

Why do they have the same \emph{column space dimension}?

\paragraph{Wrong reason: $\bm A$ and $\bm U$ has the same column space}
This is false. For example, the first column of $\bm A$ is $\begin{pmatrix}
1\\2\\-1
\end{pmatrix}\notin\col(\bm U)$. The column spaces of $\bm A$ and $\bm U$ are \emph{different}, but the dimension of them are \emph{equal}.
\paragraph{Right reason: $\bm{Ax} = \bm 0$ iff. $\bm{Ux} = \bm 0$}
The same combinations of the columns are zero (or nonzero) for $\bm A$ and $\bm U$. 

In other words, the $r$ pivot columns (for both $\bm A$ and $\bm U$) are independent; the $(n-r)$ free columns (for both $\bm A$ and $\bm U$) are dependent. 

For example, for $\bm U$, column $1$ and $3$ are ind.(pivot columns); column $2$ and $4$ are dep.(free columns).

For $\bm A$, column $1$ and $3$ are also ind.(pivot columns); column $2$ and $4$ are also dep.(free columns).
\end{example}
This example shows that \emph{Row transformation doesn't change independence relations of columns}. We give a formal proof below:
\begin{proposition}
Suppose matrix $\bm A$ is converted into $\bm B$ by row transformation. If a set of columns of $\bm A$ are ind. then so are the corresponding columns of $\bm B$.
\end{proposition}
\begin{proof}
Assume $\bm A = \left[\begin{array}{c|c|c}
a_1&\dots&a_n
\end{array}\right], \bm B = \left[\begin{array}{c|c|c}
b_1&\dots&b_n
\end{array}\right]$.

Without loss of generality (We often denote it as ``WLOG''), we assume $a_1,a_2,\dots,a_k$ are ind.(We can achieve it by switching columns.)

We define the sub-matrices $\bm{\hat{A}} = \left[\begin{array}{c|c|c}
a_1&\dots&a_k
\end{array}\right]$ and $\bm{\hat{B}} = \left[\begin{array}{c|c|c}
b_1&\dots&b_k
\end{array}\right]$.

\begin{enumerate}
\item
Notice that $\bm{\hat{A}}$ could be converted into $\bm{\hat{B}}$ by row transformation.

Hence $\bm{\hat{A}x}=\bm 0$ and $\bm{\hat{B}x}=\bm 0$ has the same solutions.
\item
On the other hand, $a_1,a_2,\dots,a_k$ are ind. columns.

Hence $\bm{\hat{A}x}=\bm 0$ has the only zero solution.
\end{enumerate}
Combining (1) and (2), $\bm{\hat{B}x}=\bm 0$ has the only zero solution. Hence $b_1,b_2,\dots,b_k$ are ind.
\end{proof}

We can answer why the coincidence shown in the example, i.e., $\bm A$ and $\bm U$ has the same column space dimension:
\begin{proposition}\label{column_rank}
Row transformation doesn't change the column rank.
\end{proposition}
\begin{proof}
Assume $\bm A\xLongrightarrow{\text{row transform}}\bm B$.

Suppose $\dim(\mathcal{C}(\bm A)) = r$, then we pick $r$ ind. columns of $\bm A$. After row transformation, they are still ind. Hence $\dim(\mathcal{C}(\bm B))\ge r = \dim(\col(\bm A))$.

Since row transformations are invertible, we get $\bm B\xLongrightarrow{\text{row transform}}\bm A$. Similarly, $\dim(\mathcal{C}(\bm A))\ge \dim(\mathcal{C}(\bm B))$.

Hence $\dim(\mathcal{C}(\bm A))= \dim(\mathcal{C}(\bm B))$.
\end{proof}
Combining proposition (\ref{row_transformation}) and (\ref{column_rank}), we can proof theorem (\ref{column_rank_row_rank}):
\begin{proof}[Proof for theorem \ref{column_rank_row_rank}]
Assume $\bm A\xLongrightarrow{\text{row transform}}\bm U$(rref).
\begin{itemize}
\item
Proposition $(\text{\ref{row_transformation}})\implies \dim(\mathcal{R}(\bm A)) = \dim(\mathcal{R}(\bm U))$.
\item
Proposition $(\text{\ref{column_rank}})\implies \dim(\mathcal{C}(\bm A) = \dim(\mathcal{C}(\bm U))$.
\item
Notice that $\dim(\mathcal{R}(\bm U))$ denotes the number of pivots, $\dim(\mathcal{C}(\bm U))$ denotes the number of pivot columns. Obviously, $\dim(\mathcal{R}(\bm U))=\dim(\mathcal{C}(\bm U))$.
\end{itemize}
Hence $\dim(\mathcal{R}(\bm A))=\dim(\mathcal{C}(\bm A))$.
\end{proof}
\begin{remark}
$\dim(\mathcal{R}(\bm U))$ essentially denotes the number of ``real'' equations. $\dim(\mathcal{C}(\bm U))$ denotes the number of ``real'' variables.

So Theorem \ref{column_rank_row_rank} implies that the number of ``real'' equations should equal to the number of ``real'' variables.
\end{remark}

\subsubsection{What is the null space dimension?}
Assume the system $\bm{Ax}=\bm b$ has $n$ variables.
\begin{proposition}For matrix $\bm A$, 
\[
\rank(\bm A) + \rank(N(\bm A)) = n.
\]
\end{proposition}
\begin{proof}
\emph{Number of pivot varibales + Number of free variables = $\bm n$}.
\end{proof}
Note that $\bm b\in\col(\bm A)$ iff. $\bm{Ax} = \bm b$ for some $\bm x$.

Hence $\mathcal{C}(\bm A)$ denotes all possible vectors in the form $\bm{Ax}$. Hence we call $\mathcal{C}(\bm A)$ as ``\emph{range space}'' of $\bm A$, which is denoted as range($\bm A$).

Equivalently, we have $\dim(\text{range($\bm A$)}) + \dim(N(\bm A)) = n$.
\begin{proposition}
If $\bm{Ax} = \bm b$ has at least one solution, then $\rank(\bm A) = \rank(\begin{bmatrix}
\bm A&\bm b
\end{bmatrix})$.
\end{proposition}
\begin{example}
Suppose $\bm A = \begin{bmatrix}
a_1&a_2&a_3
\end{bmatrix}$. If $\bm{Ax} = \bm b$ has at least one solution, then $\rank(\begin{bmatrix}
a_1&a_2&a_3
\end{bmatrix}) = \rank(\begin{bmatrix}
a_1&a_2&a_3&b
\end{bmatrix})$.
\end{example}
\begin{proof}[Proofoutline.]
\[
\bm{Ax} = \bm b\Longleftrightarrow \bm b\in\mathcal{C}(\bm A)
\]
Hence $\bm b$ is the linear combination of columns of $\bm A$. Adding one more column $\bm b$ into $\bm A$ doesn't change the dimension of $\mathcal{C}(\bm A)$. Hence $\rank(\bm A) = \rank(\begin{bmatrix}
\bm A&\bm b
\end{bmatrix})$.
\end{proof}
\begin{proposition}
If $\rank(\bm A)\le n-1$ for $m\x n$ matrix $\bm A$, then $\bm{Ax} = \bm b$ has \emph{no solution} or \emph{infinitely many solutions}.
\end{proposition}
\begin{proof}[Proofoutline.]
\[
\dim(\mathcal{C}(\bm A))+\dim(N(\bm A)) = n
\implies \dim(N(\bm A))\ge 1
\]
So we have special solutions for $\bm{Ax} = \bm b$. For the particular solution, if doesn't exist, then we have no solution, otherwise we have infinitely many solutions.
\end{proof}
\begin{definition}[Full Rank]
For $m\x n$ matrix $\bm A$, if $\rank(\bm A) = \min(m,n)$, then we say $\bm A$ is \emph{full rank}.
\end{definition}
\begin{theorem}
For $n\x n$ matrix $\bm A$, it is invertible iff. $\rank(\bm A) = n$.
\end{theorem}
\begin{proof}
\textit{Sufficiency.}
Assume $\rank(\bm A)=r<n$, then by row transformation, we can convert $\bm A$ into $\bm U := \begin{bmatrix}
\bm I_{r}&\bm B\\\bm 0&\bm 0
\end{bmatrix}$(rref), where $\bm B\in\mathbb{R}^{r\x(n-r)}$. We can represent this process in matrix notation:
\[
\bm P\bm A = \bm U:= \begin{bmatrix}
\bm I_{r}&\bm B\\\bm 0&\bm 0
\end{bmatrix},
\]
where $\bm P$ is the product of row transformation matrices, which is obviously invertible.

Since $\bm A$ is invertible, we let $\bm A^{-1} = \begin{bmatrix}
\bm C_{1}\\\bm C_{2}
\end{bmatrix}_{(r+(n-r))\times n}$. It follows that 
\[
\bm P = \bm P\bm I_{n}=\bm P(\bm A\bm A^{-1}) = (\bm{PA})\bm A^{-1} = \bm U\bm A^{-1} = \begin{bmatrix}
\bm I_{r}&\bm B\\\bm 0&\bm 0
\end{bmatrix}\begin{bmatrix}
\bm C_{1}\\\bm C_{2}
\end{bmatrix} = \begin{bmatrix}
\bm C_{1}+\bm B\bm C_{2}\\\bm 0
\end{bmatrix}.
\]
Since $\bm P$ has $(n-r)$ zero rows as shown above, it is not invertible, which is a contradiction.

\textit{Necessity.   }If $\bm A$ is full rank, then it has $n$ pivots, then by row transformation we can convert it into $\bm I$(rref). We can represent this process in matrix notation:
\[
\bm P\bm A = \bm I
\]
where $\bm P$ is the product of row transformation matrix. Hence $\bm P$ is the left inverse of $\bm A$, $\bm A$ is invertible.
\end{proof}
\subsubsection{Matrices of rank 1}
\begin{example}
\[
\bm A = \begin{bmatrix}
2&1&1\\4&2&2\\8&4&4\\-2&-1&-1
\end{bmatrix}\xlongequal{\bm v\trans = \begin{bmatrix}
2&1&1
\end{bmatrix}}\begin{bmatrix}
\bm v\trans\\2\bm v\trans\\4\bm v\trans\\-\bm v\trans
\end{bmatrix} = \begin{bmatrix}
1\\2\\4\\-1
\end{bmatrix}\bm v\trans\xlongequal{\bm u=\begin{bmatrix}
1&2&4&-1
\end{bmatrix}\trans}\bm u\bm v\trans
\]
Here $\rank(\bm A) = 1$.
\end{example}
\begin{proposition}
Every rank 1 matrix $\bm A$ has the form $\bm A = \bm u\bm v\trans = \text{column vector}\x\text{row vector}$.
\end{proposition}
You may prove it directly by SVD decomposition (we will learn it later, but note that most theorems or propositions could be proved by SVD). Alternatively, we have another proof:
\begin{proof}
We set 
\[
\bm A = \begin{bmatrix}
\bm c_1\\\bm c_2\\\vdots\\\bm c_n
\end{bmatrix},
\] 
where $\bm c_i$ is row vector. WLOG, we set $\bm c_1\ne\bm 0$ and $\bm c_1 = \begin{pmatrix}
a_1b_1&a_1b_2&\dots&a_1b_n
\end{pmatrix}$, where $a_1\ne0,$ and $b_i(i=1,\dots,n)$ are not all zero.

Since $\rank(\bm A) = 1$, we have $\dim(\mathcal{R}(\bm A)) = 1$. Hence other $\bm c_i$ are dep. with $\bm c_1$. So we set 
\[
b_i = \frac{a_i}{a_1}\mbox{ for }i=1,2,\dots,n.
\]
Thus we construct the form of $\bm A$:
\[
\bm A = \begin{bmatrix}
a_1b_1&a_1b_2&\dots&a_1b_n\\
a_2b_1&a_2b_2&\dots&a_2b_n\\
\vdots&\vdots&&\vdots\\
a_nb_1&a_nb_2&\dots&a_nb_n\\
\end{bmatrix} = \begin{bmatrix}
a_1\\a_2\\\vdots\\a_n
\end{bmatrix}\begin{bmatrix}
b_1&b_2&\dots&b_n
\end{bmatrix}
\]
\end{proof}
Question: What about the form of rank 2?

\textit{Answer: }By SVD, it has the form $\bm u_1\bm v_1\trans+\bm u_2\bm v_2\trans$.

\textit{Enjoy Your Midterm!}

\includegraphics[width =10cm]{do_it}




















