
\chapter{Week3}
\section{Monday for MAT3040}\index{Monday_lecture}
\paragraph{Reviewing}
\begin{enumerate}
\item
Complementation. 
Suppose $\dim(V)=n<\infty$, then $W\le V$ implies that there exists $W'$ such that
\[
W\oplus W'=V.
\]
\item
Given the linear transformation $T:V\to W$, define the set  $\ker(T)$ and $\text{Im}(T)$.
\item
Isomorphism of vector spaces:
$
T:V\cong W
$
\item
Rank-Nullity Theorem
\end{enumerate}
\subsection{Remarks on Isomorphism}
\begin{proposition}\label{Pro:3:1}
If $T:V\to W$ is an isomorphism, then 
\begin{enumerate}
\item
the set $\{\bm v_1,\dots,\bm v_k\}$ is linearly independent in $V$ 
if and only if $\{T\bm v_1,\dots,T\bm v_k\}$ is linearly independent.
\item
The same goes if we replace the linearly independence by spans.
\item
If $\dim(V)=n$, then $\{\bm v_1,\dots,\bm v_n\}$ forms a basis of $V$ 
if and only if $\{T\bm v_1,\dots,T\bm v_n\}$ forms a basis of $W$. 
In particular, $\dim(V)=\dim(W)$.
\item
Two vector spaces with finite dimensions are isomorphic if and only if they have the same dimension:
\end{enumerate}
\end{proposition}
\begin{proof}
It suffices to show the reverse direction. Let $\{\bm v_1,\dots,\bm v_n\}$ and $\{\bm w_1,\dots,\bm w_n\}$ be two basis of $V,W$, respectively. Define the linear transformation $T:V\to W$ by
\[
T(a_1\bm v_1+\cdots+a_n\bm v_n)=a_1\bm w_1+\cdots+a_n\bm w_n
\]
Then $T$ is surjective since $\{\bm w_1,\dots,\bm w_n\}$ spans $W$; $T$ is injective since $\{\bm w_1,\dots,\bm w_n\}$ is linearly independent.
\end{proof}

\subsection{Change of Basis and Matrix Representation}

\begin{definition}[Coordinate Vector]
Let $V$ be a finite dimensional vector space and 
$B=\{\bm v_1,\dots,\bm v_n\}$ an \emph{ordered} basis of $V$.
Any vector $\bm v\in V$ can be uniquely written as
\[
\bm v=\alpha_1\bm v_1+\cdots+\alpha_n\bm v_n,
\]
Therefore we define the map $[\cdot]_{\mathcal{B}}:V\to\mathbb{F}^n$, 
which maps any vector in $\bm v$ into its \emph{coordinate vector}:
\[
[\bm v]_{\mathcal{B}}=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}
\]
\end{definition}
\begin{remark}
Note that $\{\bm v_1,\bm v_2,\dots,\bm v_n\}$ and $\{\bm v_2,\bm v_1,\dots,\bm v_n\}$ are distinct ordered basis.
\end{remark}

\begin{example}
Given $V=M_{2\times 2}(\mathbb{F})$ and the ordered basis
\[
\mathcal{B}=
\left\{
\begin{pmatrix}
1&0\\0&0
\end{pmatrix},
\begin{pmatrix}
0&1\\0&0
\end{pmatrix},
\begin{pmatrix}
0&0\\1&0
\end{pmatrix},
\begin{pmatrix}
0&0\\0&1
\end{pmatrix},
\right\}
\]
Any matrix has the coordinate vector w.r.t. $\mathcal{B}$, i.e.,
\[
\left[
\begin{pmatrix}
1&4\\2&3
\end{pmatrix}
\right]_{\mathcal{B}}
=
\begin{pmatrix}
1\\4\\2\\3
\end{pmatrix}
\]
However, if given another ordered basis
\[
\mathcal{B}_1=
\left\{
\begin{pmatrix}
0&1\\0&0
\end{pmatrix},
\begin{pmatrix}
1&0\\0&0
\end{pmatrix},
\begin{pmatrix}
0&0\\1&0
\end{pmatrix},
\begin{pmatrix}
0&0\\0&1
\end{pmatrix},
\right\},
\]
the matrix may have the different coordinate vector w.r.t. $\mathcal{B}_1$:
\[
\left[
\begin{pmatrix}
1&4\\2&3
\end{pmatrix}
\right]_{\mathcal{B}_1}
=
\begin{pmatrix}
4\\1\\2\\3
\end{pmatrix}
\]
\end{example}

\begin{theorem}\label{The:3:1}
The mapping $[\cdot]_{\mathcal{B}}:V\to\mathbb{F}^n$ is an isomorphism.
\end{theorem}

\begin{proof}
\begin{enumerate}
\item
First show the operator $[\cdot]_{\mathcal{B}}$ is well-defined, i.e., the same input gives the same output. Suppose that
\[
\begin{array}{ll}
[\bm v]_{\mathcal{B}}=
\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}
&
[\bm v]_{\mathcal{B}}=\begin{pmatrix}
\alpha_1'\\\vdots\\\alpha_n'
\end{pmatrix},
\end{array}
\]
then we imply
\begin{align*}
\bm v&=\alpha_1\bm v_1+\cdots+\alpha_n\bm v_n\\
&=\alpha_1'\bm v_1+\cdots+\alpha_n'\bm v_n.
\end{align*}
By the uniqueness of coordinates, we imply $\alpha_i=\alpha_i'$ for $i=1,\dots,n$.
\item
It's clear that the operator $[\cdot]_{\mathcal{B}}$ is a linear transformation, i.e., 
\[
\begin{array}{ll}
[p\bm v+q\bm w]_{\mathcal{B}}=p[\bm v]_{\mathcal{B}}+q[\bm w]_{\mathcal{B}}
&
\forall p,q\in\mathbb{F}
\end{array}
\]
\item
The operator $[\cdot]_B$ is surjective:
\[
[\bm v]_{\mathcal{B}}=\begin{pmatrix}
0\\\vdots\\0
\end{pmatrix}\implies
\bm v=0\bm v_1+\cdots+0\bm v_n=\bm0.
\]
\item
The injective is clear, i.e., $[\bm v]_{\mathcal{B}}=[\bm w]_{\mathcal{B}}$ implies $\bm v=\bm w$.
\end{enumerate}
Therefore, $[\cdot]_B$ is an isomorphism.
\end{proof}


We can use the Theorem~(\ref{The:3:1}) to simplify computations in vector spaces:
\begin{example}
Given a vector sapce $V=P_3[x]$ and its basis $B=\{1,x,x^2,x^3\}$.

To check if the set $\{1+x^2,3-x^3,x-x^3\}$ is linearly independent, by part~(1) in Proposition~(\ref{Pro:3:1}) and Theorem~(\ref{The:3:1}), it suffices to check whether the corresponding coordinate vectors
\[
\left\{
\begin{pmatrix}
1\\0\\1\\0
\end{pmatrix},
\begin{pmatrix}
3\\0\\0\\-1
\end{pmatrix},
\begin{pmatrix}
0\\1\\0\\-1
\end{pmatrix}
\right\}
\]
is linearly independent, i.e., do Gaussian Elimination and check the number of pivots.
\end{example}

Here gives rise to the question: if $\mathcal{B}_1,\mathcal{B}_2$ form two basis of $V$, then how are $[\bm v]_{\mathcal{B}_1},[\bm v]_{\mathcal{B}_2}$ related to each other?

Here we consider an easy example first:

\begin{example}
Consider $V=\mathbb{R}^n$ and its basis $\mathcal{B}_1=\{\bm e_1,\dots,\bm e_n\}$. 
For any $\bm v\in V$,
\[
\bm v=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}=\alpha_n\bm e_1+\cdots+\alpha_n\bm e_n
\implies
[\bm v]_{\mathcal{B}_1}=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}
\]
Also, we can construct a different basis of $V$:
\[
\mathcal{B}_2=
\left\{
\begin{pmatrix}
1\\0\\\vdots\\0
\end{pmatrix},
\begin{pmatrix}
1\\1\\\vdots\\0
\end{pmatrix},
\ldots,
\begin{pmatrix}
1\\1\\\vdots\\1
\end{pmatrix}
\right\},
\]
which gives a different coordinate vector of $\bm v$:
\[
[\bm v]_{\mathcal{B}_2}=
\begin{pmatrix}
\alpha_1-\alpha_2\\
\alpha_2-\alpha_3\\
\vdots\\
\alpha_{n-1}-\alpha_n\\
\alpha_n
\end{pmatrix}
\]
\end{example}

\begin{proposition}[Change of Basis]
Let $\mathcal{A}=\{\bm v_1,\dots,\bm v_n\}$ and $\mathcal{A}'=\{\bm w_1,\dots,\bm w_n\}$ be two ordered basis of a vector space $V$. Define the \emph{change of basis} matrix from $\mathcal{A}$ to $\mathcal{A}'$, say $\mathcal{C}_{\mathcal{A}',\mathcal{A}}:=[\alpha_{ij}]$, where
\[
\bm v_j=\sum_{i=1}^m\alpha_{ij}\bm w_i
\]
Then for any vector $\bm v\in V$, the \textit{change of basis amounts to left-multiplying the change of basis matrix}:
\begin{equation}\label{Eq:3:1}
\mathcal{C}_{\mathcal{A}',\mathcal{A}}[\bm v]_A=[\bm v]_{A'}
\end{equation}
Define matrix $\mathcal{C}_{\mathcal{A},\mathcal{A}'}:=[\beta_{ij}]$, where 
\[
\bm w_j=\sum_{i=1}^n\beta_{ij}\bm v_i
\]
Then we imply that 
\[
(\mathcal{C}_{\mathcal{A},\mathcal{A}'})^{-1}=\mathcal{C}_{\mathcal{A}',\mathcal{A}}
\]
\end{proposition}

\begin{proof}
\begin{enumerate}
\item
First show (\ref{Eq:3:1}) holds for $\bm v=\bm v_j$, $j=1,\dots,n$:
\begin{align*}
\text{LHS of (\ref{Eq:3:1})}&=[\alpha_{ij}]\bm e_j=\begin{pmatrix}
\alpha_{1j}\\\vdots\\\alpha_{nj}
\end{pmatrix}\\
\text{RHS of (\ref{Eq:3:1})}&=[\bm v_j]_{\mathcal{A}'}=\left[\sum_{i=1}^n\alpha_i\bm w_i\right]_{\mathcal{A}'}
=
\begin{pmatrix}
\alpha_{1j}\\\vdots\\\alpha_{nj}
\end{pmatrix}
\end{align*}
Therefore, 
\begin{equation}
\mathcal{C}_{\mathcal{A}',\mathcal{A}}[\bm v_j]_{\mathcal{A}}
=[\bm v_j]_{\mathcal{A}'},\quad
\forall j=1,\dots,n.
\end{equation}
\item
Then for any $\bm v\in V$, we imply $\bm v=r_1\bm v_1+\cdots+r_n\bm v_n$, which implies that
\begin{subequations}
\begin{align}
\mathcal{C}_{\mathcal{A}',\mathcal{A}}
[\bm v]_{\mathcal{A}}
&=\mathcal{C}_{\mathcal{A}',\mathcal{A}}
[r_1\bm v_1+\cdots+r_n\bm v_n]_{\mathcal{A}}\label{Eq:3:3:b}\\
&=\mathcal{C}_{\mathcal{A}',\mathcal{A}}
\left(
r_1[\bm v_1]_A+\cdots+r_n[\bm v_n]_{\mathcal{A}}
\right)\\
&=\sum_{j=1}^nr_j\mathcal{C}_{\mathcal{A}',\mathcal{A}}[\bm v_j]_{\mathcal{A}}\\
&=\sum_{j=1}^nr_j[\bm v_j]_{\mathcal{A}'}\label{Eq:3:3:d}\\
&=\left[\sum_{j=1}^nr_j\bm v_j\right]_{\mathcal{A}'}\label{Eq:3:3:e}\\
&=[\bm v]_{\mathcal{A}'}
\end{align}
\end{subequations}
where (\ref{Eq:3:3:b}) and (\ref{Eq:3:3:e}) is by applying the lineaity of $[\cdot]_{\mathcal{A}}$ and $[\cdot]_{\mathcal{A}'}$; (\ref{Eq:3:3:d}) is by applying the result~(\ref{Eq:3:2}). Therefore (\ref{Eq:3:1}) is shown for $\forall\bm v\in V$.
\item
Now we show that $\left(
\mathcal{C}_{\mathcal{A}\mathcal{A}'}\mathcal{C}_{\mathcal{A}'\mathcal{A}}
\right)=\bm I_n$. Note that
\begin{align*}
\bm v_j&=\sum_{i=1}^n\alpha_{ij}\bm w_i\\
&=\sum_{i=1}^n\alpha_{ij}\sum_{k=1}^n\beta_{ki}\bm v_k\\
&=\sum_{k=1}^n\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)\bm v_i
\end{align*}
By the uniqueness of coordinates, we imply
\[
\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)
=\delta_{jk}:=
\left\{
\begin{aligned}
1,&\quad j=k\\
0,&\quad j\ne k
\end{aligned}
\right.
\]
By the matrix multiplication, the $(k,j)$-th entry for $\mathcal{C}_{\mathcal{A}\mathcal{A}'}\mathcal{C}_{\mathcal{A}'\mathcal{A}}$ is
\[
[\mathcal{C}_{\mathcal{A}\mathcal{A}'}\mathcal{C}_{\mathcal{A}'\mathcal{A}}]_{kj}
=
\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)
=\delta_{jk}
\implies
\left(
\mathcal{C}_{\mathcal{A}\mathcal{A}'}\mathcal{C}_{\mathcal{A}'\mathcal{A}}
\right)=\bm I_n
\]
\end{enumerate}



Noew, suppose 
\begin{align*}
\bm v_j&=\sum_{i=1}^n\alpha_{ij}\bm w_i\\
&=\sum_{i=1}^n\alpha_{ij}\sum_{k=1}^n\beta_{ki}\bm v_k\\
&=\sum_{k=1}^n\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)\bm v_i
\end{align*}
By the uniqueness of coordinates, we imply
\[
\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)
=
\left\{
\begin{aligned}
1,&\quad j=k\\
0,&\quad j\ne k
\end{aligned}
\right.
\]
where
\[
\left(
\sum_{i=1}^n\beta_{ki}\alpha_{ij}
\right)
=\left(
\mathcal{C}_{AA'}\mathcal{C}_{A'A}
\right).
\]
Therefore, $\left(
\mathcal{C}_{AA'}\mathcal{C}_{A'A}
\right)=\bm I_n$.
\end{proof}

\begin{example}
Back to Example~(3.3), write $\mathcal{B}_1,\mathcal{B}_2$ as
\[
\begin{array}{ll}
\mathcal{B}_1=\{\bm e_1,\dots,\bm e_n\},
&
\mathcal{B}_2=\{\bm w_1,\dots,\bm w_n\}
\end{array}
\]
and therefore $\bm w_i=\bm e_1+\cdots+\bm e_i$. The change of basis matrix is given by
\[
\mathcal{C}_{\mathcal{B}_1,\mathcal{B}_2}=
\begin{pmatrix}
1&1&\cdots&1\\
0&1&\cdots&1\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&1
\end{pmatrix}
\]
which implies that for $\bm v$ in the example,
\[
\mathcal{C}_{\mathcal{B}_1,\mathcal{B}_2}[\bm v]_{\mathcal{B}_2}
=
\begin{pmatrix}
1&1&\cdots&1\\
0&1&\cdots&1\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&1
\end{pmatrix}
\begin{pmatrix}
\alpha_1-\alpha_2\\\vdots\\\alpha_{n-1}-\alpha_n\\\alpha_n
\end{pmatrix}=\begin{pmatrix}
\alpha_1\\\vdots\\\alpha_n
\end{pmatrix}=[\bm v]_{\mathcal{B}_1}
\]
\end{example}

\begin{definition}
Let $T:V\to W$ be a linear transformation, and 
\[
\begin{array}{ll}
\mathcal{A}=\{\bm v_1,\dots,\bm v_m\},
&
\mathcal{B}=\{\bm w_1,\dots,\bm w_m\}
\end{array}
\]
be basis of $V$ and $W$, respectively. 
The \emph{matrix representation} of $T$ with respect to (w.r.t.) $\mathcal{A}$ and $\mathcal{B}$ is
defined as $(T)_{\mathcal{B}\mathcal{A}}:=(\alpha_{ij})\in M_{m\times m}(\mathbb{F})$, where
\[
T(\bm v_j)=\sum_{i=1}^m\alpha_{ij}\bm w_i
\]
\end{definition}











